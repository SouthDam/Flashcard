import torch
import pandas
from itertools import chain
from nltk.tokenize import sent_tokenize
from nltk import ngrams
from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    DistilBertForQuestionAnswering,
    DistilBertTokenizer,
    get_linear_schedule_with_warmup
)
class QueGenerator():
  def __init__(self,threshold=0.75):
    '''
      threshold: quality control. discard the question if overlapping < threshold
      model_dir: directory to saved models
          question generation & answer extracting models can be downloaded at https://drive.google.com/uc?id=1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V
          question answering model can be downloaded at https://huggingface.co/distilbert-base-cased-distilled-squad
            ps: tokenizer can't be downloaded directly
    '''
    self._threshold = threshold
    # self._dir = model_dir
    
    self.que_model = T5ForConditionalGeneration.from_pretrained('/t5_que_gen/t5_que_gen_model/t5_base_que_gen/')
    self.ans_model = T5ForConditionalGeneration.from_pretrained('/t5_que_gen/t5_ans_gen_model/t5_base_ans_gen/')
    self.qa_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')
    
    self.que_tokenizer = T5Tokenizer.from_pretrained('/t5_que_gen/t5_que_gen_model/t5_base_tok_que_gen/')
    self.ans_tokenizer = T5Tokenizer.from_pretrained('/t5_que_gen/t5_ans_gen_model/t5_base_tok_ans_gen/')
    self.qa_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased-distilled-squad")

      
    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    self.que_model = self.que_model.to(self.device)
    self.ans_model = self.ans_model.to(self.device)
    self.qa_model = self.qa_model.to(self.device)
  
  def generate(self, text):
    # generate questions
    answers = self._extract_answers(text)
    questions = self._get_questions(text, answers)

    # isGood: accept the question if True, deny the question if False
    output = [{'answer': ans, 'question': que, 'isGood':False} for ans, que in zip(answers, questions)]

    # questions sent to the filter to discard ques with low quality
    filtered_output = self.que_filter(output,text)

    return filtered_output
  
  def _extract_answers(self, text):
    # split into sentences
    sents = sent_tokenize(text)

    examples = []
    for i in range(len(sents)):
      input_ = ""
      for j, sent in enumerate(sents):
        if i == j:
            sent = "[HL] %s [HL]" % sent
        input_ = "%s %s" % (input_, sent)
        input_ = input_.strip()
      input_ = input_ + " </s>"
      examples.append(input_)
    
    batch = self.ans_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors="pt",truncation=True)
    with torch.no_grad():
      outs = self.ans_model.generate(input_ids=batch['input_ids'].to(self.device), 
                                attention_mask=batch['attention_mask'].to(self.device), 
                                max_length=32,
                                # num_beams = 4,
                                )
    dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]
    answers = [item.split('[SEP]') for item in dec]
    answers = chain(*answers)
    answers = [ans.strip() for ans in answers if ans != ' ']
    return answers
  
  def _get_questions(self, text, answers):
    examples = []
    for ans in answers:
      input_text = "%s [SEP] %s </s>" % (ans, text)
      examples.append(input_text)
    
    batch = self.que_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors="pt",truncation=True)
    with torch.no_grad():
      outs = self.que_model.generate(input_ids=batch['input_ids'].to(self.device), 
                                attention_mask=batch['attention_mask'].to(self.device), 
                                max_length=32,
                                num_beams = 4)
    dec = [self.que_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]
    return dec
  
  def answer_que(self, text, question):
    # qa model answers the question generated by qg model
    encoding = self.qa_tokenizer.encode_plus(question,text)
    input_ids, attention_mask = encoding["input_ids"],encoding["attention_mask"]
    start_scores, end_scores = self.qa_model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))

    ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]
    answer_tokens = self.qa_tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)

    answer = self.qa_tokenizer.convert_tokens_to_string(answer_tokens)
    
    return answer
  
  def _get_overlapping(self,ans_by_qg,ans_by_qa):
    # compute the overlapping of bigrams of two answers
    ans_by_qg_bigrams = set(ngrams(ans_by_qg,2))
    ans_by_qa_bigrams = set(ngrams(ans_by_qa,2))
    overlapping = ans_by_qg_bigrams.intersection(ans_by_qa_bigrams)
    
    overlapping_percentage = len(overlapping)/len(ans_by_qg_bigrams)
    return overlapping_percentage
  
  def que_filter(self,qa_pairs,text):
    for que_ans in qa_pairs:
      ans_by_qa = self.answer_que(text,que_ans['question'])
      overlap = self._get_overlapping(que_ans['answer'],ans_by_qa)

      if overlap > self._threshold:
        que_ans['isGood'] = True
      
    return qa_pairs

"""
###### example #######

# initiate the question generator
qg = QueGenerator(model_dir="/Users/southdam/Desktop/Master-Project-Flashcard")

text_example = '''
Mars is the fourth planet from the Sun and the second-smallest planet in 
the Solar System. It carries the name of the Roman god of war and is often 
referred to as the "Red Planet" The latter refers to the effect of iron oxide 
prevalent on Mars's surface, which gives it a reddish appearance.
'''.replace('\n','')

print(qg.generate(text_example))
"""